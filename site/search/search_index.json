{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to D-Smart ML Introduction D-SmartML is a Scala-based distributed AutoML framework built on top of spark.ml (the popular distributed big data processing framework on computing clusters). D-SmartML is equipped with a meta learning mechanism for automated algorithm selection and supports three different automated hyperparameter tuning techniques: grid search, random search and hyperband optimization. As a distributed framework, D-SmartML enables harnessing the power of computing clusters that have multiple nodes to tackle the computational complexity of the AutoML challenges on massive datasets. Main Idea The D-Smart ML library main objective is to do automatic machine learning in a distributed manner, by receiving a dataset as input and return the best Machine Learning model as output with minimum resources (ex: Time) without the need of selecting the algorithm or its hyperparameters manually. The library built on Apache Spark as a distributed processing platform, so the process of getting the best machine learning model can be easily distributed (like any spark application). The library main idea can be summarized in the below figure: The generation of the best machine learning model can be divided into two steps: Selecting the best Algorithm Selecting the best hyperparameters for the Selected Algorithm The library contains a Knowledge base to support Algorithm selection, the knowledge based created based on 80 different datasets, it contains the meta data for each dataset and its performance against each classifier. The library uses Spark ML out of the box classifiers and also contains implementation for two new Classifiers (Linear Discernment Analysis and Quadratic Discernment Analysis). The library accepts multiple parameters to determine the desired behavior and most of the parameters have default value. The process of determining best model consists of three steps as shown in the figure. Metadata extraction Algorithm Selection based on internal Knowledge base Hyperparameter optimization using (Hyperband algorithm \u2013 Random Search \u2013 Grid Search) Our Knowledgebase For each dataset, a set of meta-data extracted that represent statistics and description for this dataset, the extracted meta data can be grouped as following: For All the Dataset: Number of instances Log (Number of instances) Number of Features Log (Number of Features) Number of Classes Number of numerical Features Number of Categorical Features Ratio between Categorical & Numerical Number of Instances to Number of Features ratio Number of Missing Value Ratio of missing value For Classes: Class Entropy Classes Probabilities:(Minimum - Maximum - Standard deviation - Mean) For Categorical Features: Sum of all symbols Mean of all symbols Standard Deviation for all symbols For Numerical Features: Skewness for all Numerical features: (Max - Min - Standard Deviation - Mean) Kurtosis for all Numerical features: (Max - Min - Standard Deviation - Mean) An important characteristic adds to the knowledge base is the accuracy against machine learning classifier.The used set of classifiers are: Random Forest Logistic Regression Decision Tree Multilayer Perceptron Linear SVC Na\u00efve Bayes GBT LDA QDA We have tested each dataset in the knowledge base against each algorithm and the the Algorithm with the best Accuracy with all Algorithms within 1 Standard Deviation rang will be marked as Good, other than that will be marked as bad So, the Knowledge base contains 30 features used to represent each datasets and for each dataset we have 9 records, record per classifier, and label with two values (1 for good & 0 for bad) Algorithm Selection Since we have knowledge base contains the characteristics & the best classifier for a group of datasets, we can use machine learning to get the closest dataset in the knowledge base for an input dataset, then determine the expected best classifiers. Hyper parameter Optimization We are using Hyperband in order to determine best hyperparameters quickly, hyperband based on Successive Halving algorithm which allocates exponentially more resources to more promising configurations, the algorithm do the following: 1) Uniformly allocate a budget to a set of hyperparameter configurations 2) Evaluate the performance of all configurations 3) Throw out the worst half 4) Repeat until one configuration remains Hyperband consist of two loops: the inner loop invokes SuccessiveHalving for fixed values of (hyperparameters configurations) and resources. the outer loop iterates over different values of (hyperparameters configurations) and resources. we have used number of instances (data sampling) as the hyperband resource,so the maximum resources that can be allocated is 100% of data. Spark Implementation We have used Apache Spark to distribute the process (Feature Extraction, Algorithm Selection and hyper parameter optimization). We have also added two classifiers (LDA & QDA) to Spark ML to increase the number of the available classifiers.","title":"Home"},{"location":"#welcome-to-d-smart-ml","text":"","title":"Welcome to D-Smart ML"},{"location":"#introduction","text":"D-SmartML is a Scala-based distributed AutoML framework built on top of spark.ml (the popular distributed big data processing framework on computing clusters). D-SmartML is equipped with a meta learning mechanism for automated algorithm selection and supports three different automated hyperparameter tuning techniques: grid search, random search and hyperband optimization. As a distributed framework, D-SmartML enables harnessing the power of computing clusters that have multiple nodes to tackle the computational complexity of the AutoML challenges on massive datasets.","title":"Introduction"},{"location":"#main-idea","text":"The D-Smart ML library main objective is to do automatic machine learning in a distributed manner, by receiving a dataset as input and return the best Machine Learning model as output with minimum resources (ex: Time) without the need of selecting the algorithm or its hyperparameters manually. The library built on Apache Spark as a distributed processing platform, so the process of getting the best machine learning model can be easily distributed (like any spark application). The library main idea can be summarized in the below figure: The generation of the best machine learning model can be divided into two steps: Selecting the best Algorithm Selecting the best hyperparameters for the Selected Algorithm The library contains a Knowledge base to support Algorithm selection, the knowledge based created based on 80 different datasets, it contains the meta data for each dataset and its performance against each classifier. The library uses Spark ML out of the box classifiers and also contains implementation for two new Classifiers (Linear Discernment Analysis and Quadratic Discernment Analysis). The library accepts multiple parameters to determine the desired behavior and most of the parameters have default value. The process of determining best model consists of three steps as shown in the figure. Metadata extraction Algorithm Selection based on internal Knowledge base Hyperparameter optimization using (Hyperband algorithm \u2013 Random Search \u2013 Grid Search)","title":"Main Idea"},{"location":"#our-knowledgebase","text":"For each dataset, a set of meta-data extracted that represent statistics and description for this dataset, the extracted meta data can be grouped as following: For All the Dataset: Number of instances Log (Number of instances) Number of Features Log (Number of Features) Number of Classes Number of numerical Features Number of Categorical Features Ratio between Categorical & Numerical Number of Instances to Number of Features ratio Number of Missing Value Ratio of missing value For Classes: Class Entropy Classes Probabilities:(Minimum - Maximum - Standard deviation - Mean) For Categorical Features: Sum of all symbols Mean of all symbols Standard Deviation for all symbols For Numerical Features: Skewness for all Numerical features: (Max - Min - Standard Deviation - Mean) Kurtosis for all Numerical features: (Max - Min - Standard Deviation - Mean) An important characteristic adds to the knowledge base is the accuracy against machine learning classifier.The used set of classifiers are: Random Forest Logistic Regression Decision Tree Multilayer Perceptron Linear SVC Na\u00efve Bayes GBT LDA QDA We have tested each dataset in the knowledge base against each algorithm and the the Algorithm with the best Accuracy with all Algorithms within 1 Standard Deviation rang will be marked as Good, other than that will be marked as bad So, the Knowledge base contains 30 features used to represent each datasets and for each dataset we have 9 records, record per classifier, and label with two values (1 for good & 0 for bad)","title":"Our Knowledgebase"},{"location":"#algorithm-selection","text":"Since we have knowledge base contains the characteristics & the best classifier for a group of datasets, we can use machine learning to get the closest dataset in the knowledge base for an input dataset, then determine the expected best classifiers.","title":"Algorithm Selection"},{"location":"#hyper-parameter-optimization","text":"We are using Hyperband in order to determine best hyperparameters quickly, hyperband based on Successive Halving algorithm which allocates exponentially more resources to more promising configurations, the algorithm do the following: 1) Uniformly allocate a budget to a set of hyperparameter configurations 2) Evaluate the performance of all configurations 3) Throw out the worst half 4) Repeat until one configuration remains Hyperband consist of two loops: the inner loop invokes SuccessiveHalving for fixed values of (hyperparameters configurations) and resources. the outer loop iterates over different values of (hyperparameters configurations) and resources. we have used number of instances (data sampling) as the hyperband resource,so the maximum resources that can be allocated is 100% of data.","title":"Hyper parameter Optimization"},{"location":"#spark-implementation","text":"We have used Apache Spark to distribute the process (Feature Extraction, Algorithm Selection and hyper parameter optimization). We have also added two classifiers (LDA & QDA) to Spark ML to increase the number of the available classifiers.","title":"Spark Implementation"},{"location":"Install/","text":"Installation The library published on Maven public repo and it can be referenced easily as shown below: Maven <dependency> <groupId>com.github.DataSystemsGroupUT</groupId> <artifactId>D-SmartML</artifactId> <version>0.1</version> </dependency> Scala SBT libraryDependencies += \"com.github.DataSystemsGroupUT\" % \"D-SmartML\" % \"0.1\" Groovy Grape @Grapes( @Grab(group='com.github.DataSystemsGroupUT', module='D- SmartML', version='0.1') ) Gradle/Grails compile 'com.github.ahmed-eissa:DSmartML:0.2.4'","title":"Installation"},{"location":"Install/#installation","text":"The library published on Maven public repo and it can be referenced easily as shown below: Maven <dependency> <groupId>com.github.DataSystemsGroupUT</groupId> <artifactId>D-SmartML</artifactId> <version>0.1</version> </dependency> Scala SBT libraryDependencies += \"com.github.DataSystemsGroupUT\" % \"D-SmartML\" % \"0.1\" Groovy Grape @Grapes( @Grab(group='com.github.DataSystemsGroupUT', module='D- SmartML', version='0.1') ) Gradle/Grails compile 'com.github.ahmed-eissa:DSmartML:0.2.4'","title":"Installation"},{"location":"Start/","text":"Quick Start Example This is a complete example of using the D-Smart ML library to get the best model for a given dataset (csv file), the example does the following: - Create Spark Session - Load dataset from CSV file (as DataFrame) - Create D-Smart ML Model Selector object and call \u201cgetBestModel\u201d function. Librarry Output The below images, is a sample of the library output","title":"Quick Start"},{"location":"Start/#quick-start","text":"","title":"Quick Start"},{"location":"Start/#example","text":"This is a complete example of using the D-Smart ML library to get the best model for a given dataset (csv file), the example does the following: - Create Spark Session - Load dataset from CSV file (as DataFrame) - Create D-Smart ML Model Selector object and call \u201cgetBestModel\u201d function.","title":"Example"},{"location":"Start/#librarry-output","text":"The below images, is a sample of the library output","title":"Librarry Output"},{"location":"about/","text":"About D-Smart ML For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"About D-Smart ML"},{"location":"about/#about-d-smart-ml","text":"For full documentation visit mkdocs.org .","title":"About D-Smart ML"},{"location":"about/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"about/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"docs/","text":"Documentation HIGH LEVEL DESIGN The library has the following main component: Model Selector : Responsible for Receiving Dataset and return the best Machine learning model found within the specified time budget. It exposes a function \u201cgetBestModel\u201d that start the Algorithm selection & Hyper-parameters optimization process. Meta Data Manager : Responsible for extracting the Dataset metadata (the data set characteristics) and produce a metadata object represent the dataset characteristic The created object similar to Knowledgebase instances KB Manager : Responsible for handling all Knowledgebase activates like (Load Knowledge base, update knowledge base, \u2026) But the most important role, is to determine the suitable algorithms based on the dataset meta data and the loaded Knowledge base Classifier Manager : Represent all Classifiers and their parameters. It contains the distribution for each parameter, to be used in random search and hyperband It contains hyper-parameters range to be used with Grid Search KB Model : The mode that has been built based on the knowledge base and use to predict the suitable classifier(s) Grid Search : Responsible for doing Grid Search algorithm to do hyper parameter optimization Random Search : Responsible for doing Random Search algorithm to do hyper parameter optimization Hyperband : Responsible for doing hyperband algorithm to do hyper parameter optimization PROCESSING SEQUENCE To get the best model for input dataset, the library executes the following sequence: \u201cModel Selector\u201d receive the dataset and call \u201cKB Manger\u201d to determine best classifiers suitable to the dataset. \u201cKB Manager\u201d Load KB Model and call \u201cMeta data manger\u201d to extract metadata \u201cKB Manager\u201d receive Metadata object and use the loaded model to predict suitable classifier then return Classifier List \u201cModel Selector\u201d loop on the classifiers list and call \u201cClassifier manger\u201d to get the hyperparameters for each classifier (and their values distribution or grid) \u201cModel Selector\u201d call \u201cHyperband\u201d or \u201cRandom Search\u201d or \u201cGrid Search\u201d and send the Classifier and its hyper parameters (distribution or grid) Parameters Parameter Description Data Type Default Value eta an input that controls the proportion of configurations discarded in each round of SuccessiveHalving (in hyperband) Integer 5 Max Data Percentage the maximum amount of resource that can be allocated to a single configuration Integer 100 Parallelism the maximum amount of resource that can be allocated to a single configuration (models will only be run in parallel if there are enough resources available in the cluster. Otherwise, models will be queued in the Spark scheduler and have to wait for the current jobs to complete before being run.) Integer 1 Try N Classifier Maximum Number of Algorithms should be checked (out of the best algorithms based on the kB) Integer 2 Max Time Maximum Time allowed for hyper parameter optimization (per each Algorithm) to get the best hyperparameter values (in Seconds) Integer 1800 HP Optimizer Hyper parameters optimizer (1: Random Search or 2: Hyperband) Integer 2 Convert To Vector Assembly If the input dataset features need to be converted to Vector or not Boolean false","title":"Documentation"},{"location":"docs/#documentation","text":"","title":"Documentation"},{"location":"docs/#high-level-design","text":"The library has the following main component: Model Selector : Responsible for Receiving Dataset and return the best Machine learning model found within the specified time budget. It exposes a function \u201cgetBestModel\u201d that start the Algorithm selection & Hyper-parameters optimization process. Meta Data Manager : Responsible for extracting the Dataset metadata (the data set characteristics) and produce a metadata object represent the dataset characteristic The created object similar to Knowledgebase instances KB Manager : Responsible for handling all Knowledgebase activates like (Load Knowledge base, update knowledge base, \u2026) But the most important role, is to determine the suitable algorithms based on the dataset meta data and the loaded Knowledge base Classifier Manager : Represent all Classifiers and their parameters. It contains the distribution for each parameter, to be used in random search and hyperband It contains hyper-parameters range to be used with Grid Search KB Model : The mode that has been built based on the knowledge base and use to predict the suitable classifier(s) Grid Search : Responsible for doing Grid Search algorithm to do hyper parameter optimization Random Search : Responsible for doing Random Search algorithm to do hyper parameter optimization Hyperband : Responsible for doing hyperband algorithm to do hyper parameter optimization","title":"HIGH LEVEL DESIGN"},{"location":"docs/#processing-sequence","text":"To get the best model for input dataset, the library executes the following sequence: \u201cModel Selector\u201d receive the dataset and call \u201cKB Manger\u201d to determine best classifiers suitable to the dataset. \u201cKB Manager\u201d Load KB Model and call \u201cMeta data manger\u201d to extract metadata \u201cKB Manager\u201d receive Metadata object and use the loaded model to predict suitable classifier then return Classifier List \u201cModel Selector\u201d loop on the classifiers list and call \u201cClassifier manger\u201d to get the hyperparameters for each classifier (and their values distribution or grid) \u201cModel Selector\u201d call \u201cHyperband\u201d or \u201cRandom Search\u201d or \u201cGrid Search\u201d and send the Classifier and its hyper parameters (distribution or grid)","title":"PROCESSING SEQUENCE"},{"location":"docs/#parameters","text":"Parameter Description Data Type Default Value eta an input that controls the proportion of configurations discarded in each round of SuccessiveHalving (in hyperband) Integer 5 Max Data Percentage the maximum amount of resource that can be allocated to a single configuration Integer 100 Parallelism the maximum amount of resource that can be allocated to a single configuration (models will only be run in parallel if there are enough resources available in the cluster. Otherwise, models will be queued in the Spark scheduler and have to wait for the current jobs to complete before being run.) Integer 1 Try N Classifier Maximum Number of Algorithms should be checked (out of the best algorithms based on the kB) Integer 2 Max Time Maximum Time allowed for hyper parameter optimization (per each Algorithm) to get the best hyperparameter values (in Seconds) Integer 1800 HP Optimizer Hyper parameters optimizer (1: Random Search or 2: Hyperband) Integer 2 Convert To Vector Assembly If the input dataset features need to be converted to Vector or not Boolean false","title":"Parameters"},{"location":"performance/","text":"Performance","title":"Performance"},{"location":"performance/#performance","text":"","title":"Performance"}]}